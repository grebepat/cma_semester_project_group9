---
title: Cycling Secrets
subtitle: Decoding Bike Messenger Paths for Efficiency
author: Maurin Huonder and Patrick Greber
date: "18.06.2024"
format: 
  html:
    code-fold: true
execute:
  warning: false
  message: false
  cache:   true
lang: en  
bibliography: bibliography.bib
---

# Install Packages

```{r install required packages}

#| echo: false
#| warning: false
#| message: false
#| results: 'hide'

options(repos = c(CRAN = "https://cran.rstudio.com"))

install.packages("pacman")
library("pacman")


p_install("dplyr", force = FALSE)
p_install("ggplot2", force = FALSE)
p_install("readr", force = FALSE)
p_install("tidyr", force = FALSE)
p_install("sf", force = FALSE)
p_install("terra", force = FALSE)
p_install("tmap", force = FALSE)
p_install("zoo", force = FALSE)
p_install("units", force = FALSE)
p_install("patchwork", force = FALSE)
p_install("tidyverse", force = FALSE)
p_install("leaflet", force = FALSE)
p_install("shiny", force = FALSE)
p_install("XML", force = FALSE)
p_install("lubridate", force = FALSE)
p_install("ggh4x", force = FALSE)
p_install("forcats", force = FALSE)
p_install("purrr", force = FALSE)
p_install("viridis", force = FALSE)
p_install("httr", force = FALSE)
p_install("hereR", force = FALSE)
p_install("sp", force = FALSE)
p_install("ggpubr", force = TRUE)
p_install("png", force = FALSE)
p_install("grid", force = FALSE)
p_install("raster", force = FALSE)
p_install("gridExtra", force = FALSE)
p_install("RColorBrewer", force = FALSE)


```

# Load libraries

Load necessary libraries

```{r load necessary libraries}

#| echo: false
#| warning: false
#| message: false
#| results: 'hide'

library("ggh4x")
library("dplyr")
library("ggplot2")
library("tidyr")
library("sf")
library("sp")
library("terra")
library("tmap")
library("zoo")
library("tidyverse")
library("leaflet")
library("XML")
library("lubridate")
library("forcats")
library("httr")
library("hereR")
library("png")
library("gridExtra")
library("RColorBrewer")
library("purrr")


```

# Task 1: Import Express: Bringing GPS Data Onboard

Import gps data from bike messengers

```{r import raw gps data}

#| echo: false
#| warning: false
#| message: false
#| results: 'hide'


## generate a list of all filenames including the path from the subfolder they are stored in
file <- list.files("gps_files_shared", recursive = TRUE, pattern = "\\.gpx$", full.names = TRUE)


## Function to extract messenger and id from file path
extract_info <- function(file) {
  messenger <- as.factor(gsub(".*/gps_files_([^/]+)/.*", "\\1", file))
  id <- as.factor(paste(gsub('.*/(.*).gpx','\\1', file), gsub(".*/gps_files_([^/]+)/.*", "\\1", file), sep = "_"))
  list(messenger = messenger, id = id)
}


## Function to process each file
process_file <- function(file) {
  df <- st_read(file, "track_points")
  
  info <- extract_info(file)
  df$messenger <- info$messenger
  df$id <- info$id

  df_sf <- st_as_sf(df, coords = c("lon", "lat"), crs = 4326, remove = FALSE)
  df_sf <- st_transform(df_sf, crs = 2056)
  df_sf$shift <- gsub('.*/(.*).gpx','\\1', file)
  df_sf$x <- st_coordinates(df_sf)[,1]
  df_sf$y <- st_coordinates(df_sf)[,2]
  df_sf <- select(df_sf, id, messenger, shift, time, x, y, ele, geometry)
  df_sf$origin <- file
  df_sf
}


## Apply the function to each file using purrr's map function
single_routes <- purrr::map(file, process_file)


## Combine all results
all_routes <- do.call(rbind, single_routes)
save(all_routes, file = "all_routes.rda")


## Seperate file into raeubertochter and donner and save output for visualizations under preprocessing.qmd
raeubertochter_raw <- filter(all_routes, messenger == "raeubertochter")
save(raeubertochter_raw, file = "raeubertochter_raw.rda")

donner_raw <- filter(all_routes, messenger == "donner")
save(donner_raw, file = "donner_raw.rda")


## Our raw data covers one shift per messenger, a total of 11'699 fixes were recorded:
### raeubertochter: 23.10.2023, 12:59:44 - 17:51:21, 7102 fixes
### donner: 30.01.2024, 10:21:41 - 22:14:27, 4597 fixes

```

::: {#fig-raw_data layout-ncol="2"}
![Räubertochter](raeubertochter_raw.png) ![Donner](donner_raw.png)

RAW DATA
:::

# Task 2: Import Spatial Data

Import spatial data. Geopackage was preprocessed using Quantum GIS Version 3.34.5.

```{r import spatial data}

#| echo: false
#| warning: false
#| message: false
#| results: 'hide'

## All Spatial Data is stored in a geopackage called basic_data. Basic_data consists of several layers such as the street network, surface type or housing footprint. The layers were preprocessed and cliped to the extent of the city outline of zurich using Quantum GIS Version 3.34.5


## Show layers in basic_data.gpkg
st_layers("gis_files/basic_data.gpkg")


## Import street network from zurich, based on the swisstlm3d
streets <- read_sf("gis_files/basic_data.gpkg", "street_network_z") |> 
   select(objektart, geom) |> 
  mutate(
    objektart = as.factor(objektart),
    width = as.numeric(substr(objektart, start = 1, stop = 1)),
  ) |> 
  na.omit()


## Import city border of zurich
outline <- read_sf("gis_files/basic_data.gpkg", "city_outline")


## Import housing footprint of zurich
housing <- read_sf("gis_files/basic_data.gpkg", "housing_footprint") |> 
   select(objektart, geom) |>
  mutate(
    objektart = as.factor(objektart)
  )


## Import surface type of zurich
surface <- read_sf("gis_files/basic_data.gpkg", "surface_type") |> 
   select(art, geom) |> 
  mutate(
    art = as.factor(art)) |> 
 filter(art == "fliessendes Gewässer" | art == "stehendes Gewässer" | art == "Strasse, Weg" | art == "Verkehrsinsel") |> 
  na.omit()


## Import digital height model DHM25, a set of data representing the 3D form of the earth’s surface without vegetation and buildings
height <- terra::rast("gis_files/dhm25_zh.tif")


```

# Task 3: Signal Sync: Assessing Sampling Intervals Across Messenger GPS Systems

```{r assessing sampling intervals}

## If a larger sampling grid is needed: selecting every 10th row from  movement data:
###all_routes <- all_routes[seq(from = 1, to = #nrow(all_routes), by = 5), ]


## calculate rowwise time difference
all_routes <- all_routes |> 
    group_by(id) |> 
    mutate(
    time_difference = as.numeric(difftime(time, lag(time), units = "secs"))) |>
    ungroup()


## How do the time difference differ between messengers?
all_routes |> 
  group_by(id) |> 
  filter(time_difference <= 30) |> # remove outliers to get a clearer view on the average sampling interval
  summarise(
    mean <- mean(time_difference, na.rm = T)
    )

## Both with similar time difference between fixes, raeubertochter with slightly shorter intervals


## max value between two fixes, in min
max(all_routes$time_difference, na.rm = T) / 60

## max time difference at 50min

```

Visualize sampling interval

```{r visualize sampling intervals via ggplot}



## Quick overview how time differences distribute
p_sampling_interval <- ggplot(all_routes, aes(x = time_difference)) +
  geom_histogram(binwidth = 0.25, col = "white", fill = "grey25") +
  scale_y_log10() +
  scale_x_log10() +
  labs(x = "Time Difference in sec. (Log Scale)", y = "Count (Log Scale)") +
  ggtitle("Histogram of Time Differences with Log Y Axis and Log X Axis") +
  facet_wrap(all_routes$id) +
  theme_minimal()


## show histogram
print(p_sampling_interval)

# Both with similar but slightly different sampling regimes, raeubertochter with an intervall of ~1 second. It seems that static time is already removed in here dataset -> strava data! Most time differences between 1 and 10 seconds, some outliers at more than 1000 seconds

# Donner with more variation between fixes, but still short intervals, might need another segmentation...

```

# Task 4: From Dots to Drops: Segmenting GPS Fixes into Deliveries

```{r segmenting deliveries}

## For seperating the gps data into different segments, we analyse time differences between them. As strava data already removes most of the static points we're in no need to calculate mean_step's but can solely relay on big time gaps.


## create a moving time window
all_routes_seg <- all_routes |> 
   group_by(id) |> 
   mutate(
        nMinus2 = difftime(time, lag(time, 2)), 
        nMinus1 = difftime(time, lag(time,1)),  
        nPlus1  = difftime(lead(time, 1), time), 
        nPlus2  = difftime(lead(time, 2), time)  
    )


## calculate rowwise mean distance per messenger
all_routes_seg <- all_routes_seg |> 
    group_by(id) |>
    mutate(
        timeMean = (nMinus2 + nMinus1 + nPlus1 + nPlus2) / 4
    ) |>
    ungroup()


## create a new column static, based on time_difference (over 20s time difference)
all_routes_seg <- all_routes_seg |> 
  mutate(new_segment = timeMean > 20)

## As strava already removed most static points, we're able to seperate segments via time_difference only. We work with a treshold t of 20 seconds.


## it assigns unique IDs based on the column static
rle_id <- function(vec) {
    x <- rle(vec)$lengths
    as.factor(rep(seq_along(x), times = x))
}


## removes static rows
all_routes_seg <- all_routes_seg |>
    mutate(temp_id = rle_id(new_segment)) |> 
    filter(!new_segment)


## remove segments shorter than two minuntes
all_routes_seg <- all_routes_seg |> 
  group_by(temp_id) |> 
  mutate(duration = difftime(max(time), min(time))
  ) |> 
  filter(!duration < 120) |> 
  ungroup()


## Assining new segment_id starting at one, credits to: https://stackoverflow.com/questions/39650511/r-group-by-variable-and-then-assign-a-unique-id
all_routes_seg <- all_routes_seg |> 
  group_by(temp_id, messenger) |> 
  mutate(segment_id = as_factor(cur_group_id())) |> 
  ungroup() |> 
  select(-temp_id)

save(all_routes_seg, file = "all_routes_seg.rda")


## how many segments have been differentiated?
all_routes_seg |> 
  group_by(messenger) |> 
  summarise(length(unique(segment_id)))

## donner with 36 deliveries
## raeubertochter with 22 deliveries


## filter segments according to messenger
raeubertochter_seg <- filter(all_routes_seg, messenger == "raeubertochter")
save(raeubertochter_seg, file = "raeubertochter_seg.rda")

donner_seg <- filter(all_routes_seg, messenger == "donner")
save(donner_seg, file = "donner_seg.rda")


```

# Task 5: Lost in Transit: Tackling Tunnel GPS Blackouts for Bike Messenger

```{r tackling tunnel gps blackouts}

## problem: we loose GPS signal in the bicycle tunnel from Enge to Sihlhölzli. Our segmentation splits the trajectories into two separate deliveries, even though it's the same route. We try to recognize split segments that we're falsely segmented:
all_routes_seg_tunnel <- all_routes_seg


## extract first and last points of each segment
first_last_points <- all_routes_seg_tunnel |> 
  group_by(segment_id) |> 
  slice(c(1, n())) |> 
  ungroup()


## Create entry and exit points at the tunnel. For future project, there might be a data set with tunnel entities, where we could extract the entry and exit points automatically
tunnel_exit <- st_sfc(st_point(c(2682368, 1246996)), crs = 2056)
tunnel_entry <- st_sfc(st_point(c(2682592, 1246751)), crs = 2056)


## create a buffer around the tunnel entry and exit,
# tunnel_points <- st_sfc(tunnel_entry, tunnel_exit, crs = 2056)
buffer_entry <- st_buffer(tunnel_entry, dist = 50)
buffer_exit <- st_buffer(tunnel_exit, dist = 50)


## Identify segments within the buffer
first_last_points_within <- first_last_points |> 
  mutate(
    intersect_start = sapply(st_intersects(geometry, buffer_entry, sparse = FALSE), any),
    intersect_end = sapply(st_intersects(geometry, buffer_exit, sparse = FALSE), any)
  ) |> 
  filter(intersect_start | intersect_end)


## Calculate time difference between end of one segment and start of the next
first_last_points_within <- first_last_points_within |> 
  arrange(segment_id, time) |> 
  mutate(time_diff = time - lag(time))


## Match segments where time difference is less than 2 minutes -> falsely separated segments!
matched_segments <- first_last_points_within |> 
  filter(time_diff <= 120 | is.na(time_diff) ) |> 
  mutate(new_segment_id = lag(segment_id)) |> 
  select(x,y,geometry,time_diff, segment_id, new_segment_id)

## recognize start- and endpoints of segments that lay within the buffer and are only separated by 2 minutes

########## FOR FUTURE PROJECTS -> try to implement matching via segment_id, not time difference! If we make the assumption that we separate trajectories in the tunnel, we can assume that falsely separated segments follow each other. Segments were falsely separated when an endpoint of segment_id 13 and a starting point of segment_id 14 lay within the corresponding buffers. Would be a much cleaner approach! 


## create a table with falsely seperated segment_id's
lookup_table <- matched_segments |> 
  filter(!is.na(new_segment_id) & segment_id != new_segment_id) |> # filter rows where new and old segment_id's do not match
  select(segment_id, new_segment_id)  |> 
  distinct() # replace duplicates


## We recognized one falsely separated trajecotry. Segment 49 and 50 we're separated but should be kept together


## In this case, we could merge the segments back together manually. but with larger dataset, this might not be feasible. That's why we try a more general approach:


## create a lookup_vector based on our lookup_table. credits go to: https://stackoverflow.com/questions/35636315/replace-values-in-a-dataframe-based-on-lookup-


## This transforms the lookup table into a vector where the names are segment_id and the values are new_segment_id. This vector will be used to quickly find and replace old segment IDs with new ones.
lookup_vector <- setNames(lookup_table$new_segment_id, lookup_table$segment_id)


## match segment_id's with our lookup_vector. For each segment_id in all_routes_seg_tunnel, it checks if that ID is in the lookup vector. If it is, it replaces it with the corresponding new_segment_id from the lookup vector; if not, it keeps the original segment_id
all_routes_seg_tunnel$segment_id_new <- ifelse(all_routes_seg_tunnel$segment_id %in% names(lookup_vector), lookup_vector[match(all_routes_seg_tunnel$segment_id, names(lookup_vector))], all_routes_seg_tunnel$segment_id)


## tidy up our corrected dataframe
all_routes_seg_tunnel_cor <- all_routes_seg_tunnel |> 
  group_by(segment_id_new, messenger) |> 
  mutate(segment_id_cor = as_factor(cur_group_id())) |> 
  select(-segment_id, -segment_id_new) |> 
  ungroup()


## How many deliveries after the correction?
all_routes_seg_tunnel_cor |> 
  group_by(messenger) |> 
  summarise(length(unique(segment_id_cor)))


save(all_routes_seg_tunnel_cor, file = "all_routes_seg_tunnel_cor.rda")


## donner with 36 deliveries
## raeubertochter with 21 deliveries


## seperate into raeubertochter and save for later visualization in preprocessing.qmd
raeubertochter_cor <- filter(all_routes_seg_tunnel_cor, messenger == "raeubertochter")
save(raeubertochter_cor, file = "raeubertochter_cor.rda")


## and donner
donner_cor <- filter(all_routes_seg_tunnel_cor, messenger == "donner")
save(donner_cor, file = "donner_cor.rda")

```


# Task 6: Creating alternative trajectories with hereR:route()

```{r alternative routing}

## Alternatives were created in prepocessing.qmd, a file calles alternative.rda is stored in the global environment
# load alternatives

load("C:/Users/patri/Desktop/Semesterproject/semesterproject/alternative.rda")


```


# Task 7: Combining Trajectories and Alternatives into a single Data Frame

```{r combined dataset}

## simplify original dataframe
original <- all_routes_seg_tunnel_cor |> 
  select(messenger, time, geometry, segment_id_cor) |> 
  rename(segment_id = segment_id_cor) |> 
  mutate(
    type = as.character("original"),
    segment_id = as.numeric(segment_id)
    )

  
## simplify alternative datast
alternative <- alternative |> 
  select(segment_id, rank,  geometry) |> 
  st_transform(crs = 2056) |> 
  mutate(
    type = as.character("alternative"),
    segment_id = as.numeric(segment_id)
  )

#combine the dataset
combined <- bind_rows(original, alternative)


# fill in missing messenger values
combined <- combined |> 
  arrange(segment_id, messenger) |> 
  fill(messenger, .direction = "down")

combined$rank[is.na(combined$rank)] <- 0


## Save combined dataset for visualizations in preprocessing.qmd
save(combined, file = "combined.rda")

```

# Filter matching segment id's

```{r}

## As mentioned above, we have the problem, that we only had limited access to the here API. Therefore we do not have an alternative for every single original trajectory. Before continuoing with our calculations, we need to filter the segments that each have an original route and an alternative


## get every unique segment id from the type original
original_ids <- combined |> 
  filter(type == 'original') |> 
  pull(segment_id) |> 
  unique()


## get every unique segment id from the type alternative
alternative_ids <- combined |>  
  filter(type == 'alternative') |>  
  pull(segment_id) |> 
  unique()


## Find common segment_ids, how many segments are there?
common_ids <- intersect(original_ids, alternative_ids)

length(unique(common_ids))

######### ATTENTION: Whenever we compute alternatives with the API from here, other alternatives are generated and some are not. This changes with every computation!!! 

## Latest computation: 50 matching segments


## Filter the dataframe
filtered_df <- combined |> 
  filter(segment_id %in% common_ids)


# We have a very uneven distribution of gps fixes, a problem we'll solve later

```

# Task 8: Map Matching

```{r map matching based on Nils method}

## since we want to find the closest location on the road over ALL roads
## we need to create a union of the roads first.
street_network <- st_union(streets)


# Now we can get the nearest point for each GPS location
nearest <- st_nearest_points(filtered_df, street_network)
save(nearest, file = "nearest.rda")

# The output is a line for each point
# Now we need convert the output from LINE to POINT. 
# This doubles the number of features
near_p <- st_cast(nearest, "POINT")


# now we subset the points. Even numbers are the new, mapmatched points.
near_to <- near_p[c(FALSE,TRUE)]


# Update the geometry of the original points with the new map matched locations
st_geometry(filtered_df) <- st_geometry(near_to)


save(filtered_df, file = "filtered_df.rda")

```

# Problem: Uneven distribution of data points between original and alternative

```{r uneven distribution gps fixes}


## We encounter the problem, that our alternative routes consist of two to three times less data points as our original routes. The denser sampling intervals in our original routes could lead to uneven calculations. For example, smaller intervals could detect more differences in height/slope than the wider sampling interval in the alternative routing


## how many fixes?
overview <- filtered_df |> 
  st_drop_geometry() |> 
  group_by(type, messenger, segment_id, rank) |> 
  summarise(count = n(), 
            count2 = n_distinct(segment_id)) |> 
  mutate(
    type = as.factor(type),
    count = as.numeric(count)
  ) |> 
  na.omit()


overview <- overview |> 
  group_by(type) |> 
  arrange(type) |> 
  select(type, messenger, count, rank) |> 
  as.data.frame()

overview |> 
  group_by(type, rank) |> 
  summarise(
    mean = mean(count)
  )

ggplot(df) +
  geom_boxplot(aes(x = type, y = count))


```

# Task 9: Interpolating Data Frame

```{r Interpolating trajectories}

## Because of the uneven distribution, we decided to interpolate along the original and alternative trajectories


## simplify dataset
filtered_df <- select(filtered_df, -time)


## transform the combined dataset into a linestring. The interpolation will follow these linestrings later
df_lines2 <- filtered_df |> 
  group_by(type, messenger, segment_id, rank) |> 
  summarise(geometry = st_combine(geometry)) |> 
  st_cast("LINESTRING")


## interpolating
## creating a function to interpolate points along lines, there are several functions for interpolation like st_line_sample, sp_sample or gInterpolate by rgeos. 
interpolate_points <- function(geometry, dist=10) {
  len <- st_length(geometry) # how long is the line?
  n_points <- ceiling(len / dist) # how many points will be sampled -> tot_length / distance between points
  st_line_sample(geometry, sample = seq(0, 1, length.out = n_points)) # sequence between 0 and 1 indicating how many points will be sampled based on the length.out = n_points which represents how many points will be sampled
}


# Apply the function to each LINESTRING
# purrr::map: This is a function from the purrr package. The map function applies a function to each element of a list or vector and returns a list. Credits go to Nils ;)
df_lines2$points <- purrr::map(df_lines2$geometry, interpolate_points)


## Unnest the points
df_points <- tidyr::unnest(df_lines2, points)


## create linestring out of interpolated points for later visualizations
df_lines_interp <- df_points |> 
  st_drop_geometry(points)


## remove LINEGEOMETRY
df_points <- df_points |> 
  st_drop_geometry(geometry)


## cast POINTGEOMETRY
df_sf <- st_as_sf(df_points)


## Use st_cast to convert multipoint to single point
df_single_points <- st_cast(df_sf, "POINT")


## somehow have troubles with the geometries, try to solve it:
df_single_points <- st_set_crs(df_single_points, 2056)
df_single_points <- st_transform(df_single_points, crs = 2056)


save(df_single_points, file = "df_single_points.rda")

## how many fixes for alternative / original?


## our interpolation generated a dataset with a even distribution between alternatives and originals routes:
## original: 
## alternative: 


```


# Task 9: Enrich network

```{r}

## We work with 52 trajectories (donner: 33 / raeubertochter: 19)
# Enrich combined dataset with street and environmental data

## buffer point features to intersect them with the street network
traj <- st_buffer(df_single_points, 0.5)


## set crs for polygon layer
traj <- st_set_crs(traj, 2056)


## intersect polygon with street network
traj_width <- st_join(traj, streets, left = TRUE, suffix = "street", join = st_intersects)


## transfrom back to a point layer
traj_width <-  st_centroid(traj_width, crs = 2056)


## Fill missing width's in dataframe
traj_width <- traj_width |> 
  arrange(type, messenger, segment_id, rank) |> 
  fill(objektart, .direction = "down") |> 
  fill(width, .direction = "down")


## Extract height at every point using dhm25
extracted_height <- extract(height, traj_width)
traj_width$height <- round(extracted_height$dhm25_zh,2)


## Extract slope at every point
slope <- terrain(height, v = "slope", unit = "degrees", neighbors = 8)
extracted_slope <- extract(slope, traj_width)
traj_width$slope <- round(extracted_slope$slope,2)


## simplify dataframe
traj_enriched <- traj_width |> 
  select(-objektart)


## Calculations between a pair of points
traj_enriched_final <- traj_enriched |> 
  arrange(type, segment_id, rank) |> 
  group_by(type, segment_id, rank) |> 
  mutate(lead_geom = lead(points),
         travel_dist = sqrt(round(ifelse(!is.na(lead_geom), st_distance(lead_geom, points, by_element = TRUE), 0), 1))^2,
         dist_cumulative = cumsum(ifelse(travel_dist > 0, travel_dist, 0)), 
         tot_segment_length = max(dist_cumulative, na.rm = T),
         height_diff = c(0, diff(height)),
         tot_height_up = sum(ifelse(height_diff > 0, height_diff, 0)),
         tot_height_down = sum(ifelse(height_diff < 0, height_diff, 0)),
         slope_calc = ifelse(travel_dist != 0, (height_diff / travel_dist)*100, 0)
         ) 


## Analysing result
false <- traj_enriched_final |> 
  filter(segment_id == 4 & type == "original", travel_dist < 4.01)
blabla <- traj_enriched_final |> 
  filter(segment_id == 4 & type == "original", travel_dist > 5) 
haha <- df_lines2 |> 
  filter(segment_id == 4 & type == "original")
  
tmap_mode("view")
tm_shape(false) +
tm_dots(col = "gold", size = 0.15)+
tm_shape(blabla) +
  tm_dots(col = "travel_dist") +
  tm_shape(haha) +
  tm_lines()


## somehow the interpolation did not work as expected, there are some overlaying points resulting in a travel_dist of 0.. also euclid distance results in travel_dist shorter than 10m because of turning angles.. we clean the dataset as follows:

# 1.) remove poits with a euclid distance of < 4.5m
# 2.) set travel dist to 10m manually


## Cleaning
traj_cleaned <- traj_enriched_final |> 
  filter(!travel_dist < 4.5) 

  
# repeat calculations
new <- traj_cleaned |> 
  arrange(type, segment_id, rank) |> 
  group_by(type, segment_id, rank) |> 
  mutate(lead_geom = lead(points),
         travel_dist = sqrt(round(ifelse(!is.na(lead_geom), st_distance(lead_geom, points, by_element = TRUE), 0), 1))^2,
         dist_cumulative = cumsum(ifelse(travel_dist > 0, travel_dist, 0)), 
         tot_segment_length = max(dist_cumulative, na.rm = T),
         height_diff = c(0, diff(height)),
         tot_height_up = sum(ifelse(height_diff > 0, height_diff, 0)),
         tot_height_down = sum(ifelse(height_diff < 0, height_diff, 0)),
         slope_calc = ifelse(travel_dist != 0, (height_diff / travel_dist)*100, 0)
         ) |> 
  na.omit()


# setting travel distance to 10m manually, traveled distance was 10m on the line, so euclidiean distance is a little useless here..
new$travel_dist = 10

  

## Calculations over a moving window of 100m (travel distance, 10 points)
## Indexing 100m segments
new <- new |> 
group_by(type, messenger, segment_id, rank) |> 
mutate(
    index = as.integer(cumsum(travel_dist) / 100.1 + 1)
 )


## Transform certain columns
new <- new |> 
  mutate(
    type = as.factor(type),
    segment_id = as.factor(segment_id),
    rank = as.factor(rank),
    index = as.factor(index),
    width = as.factor(width)
  )


## Height differences over first and last point of 100m segments
new <- new |> 
group_by(type, messenger, segment_id, rank, index) |> 
mutate(
   height_diff_100 = last(height) - first(height),
   n = n()
    )
 

alternative_final <- new |> 
      filter(type == "alternative")
    
    
original_final <- new |> 
      filter(type == "original")

```

# Task 10: Results

Adjust this table after each cycle...

|                           |       | original |       |       | alternative |       |       |
|---------|---------|:--------|:--------|:--------|:--------|:--------|:--------|
| variable                  | unit  | min      | mean  | max   | min         | mean  | max   |
| travel distance           | \[m\] | 584      | 2222  | 4039  | 360         | 2315  | 4089  |
| average gradient per 100m | \[%\] | -8.88    | 0.03  | 11.8  | -11.7       | 0.05  | 11.8  |
| max. gradient per 100m    | \[%\] | \-       | 4.09  | 75.0  | \-          | 3.89  | 66.7  |
| height up                 | \[m\] | 0.68     | 19.4  | 87.7  | 0.78        | 19.1  | 83.1  |
| height down               | \[m\] | -0.25    | -18.9 | -67.2 | -0.39       | -19.1 | -61.9 |

```{r}

## Descriptive analysis - segment length
new |> 
  st_drop_geometry() |> 
  group_by(type) |> 
  summarise(Mittelwert = mean(tot_segment_length), Minimum = min(tot_segment_length), Maximum = max(tot_segment_length)
            , Standardabweichung = sd(tot_segment_length), Bereich = (max(tot_segment_length)-min(tot_segment_length)), median(tot_segment_length))


## Descriptive analysis - height up
new |> 
  st_drop_geometry() |> 
  group_by(type) |> 
  summarise(Mittelwert = mean(tot_height_up), Minimum = min(tot_height_up), Maximum = max(tot_height_up)
            , Standardabweichung = sd(tot_height_up), Bereich = (max(tot_height_up)-min(tot_height_up)), median(tot_height_up))


## Descriptive analysis - height down
new |> 
  st_drop_geometry() |> 
  group_by(type) |> 
  summarise(Mittelwert = mean(tot_height_down), Minimum = min(tot_height_down), Maximum = max(tot_height_down)
            , Standardabweichung = sd(tot_height_down), Bereich = (max(tot_height_down)-min(tot_height_down)), median(tot_height_down))


## summarise values over 100m subsegments
results <- new %>%
  group_by(type, segment_id, index) %>%
  na.omit() |> 
  summarise(
    n = n(),
     slope_down = mean(ifelse(slope_calc < 0, slope_calc, 0), na.rm = TRUE),
     slope_up = mean(ifelse(slope_calc > 0, slope_calc, 0), na.rm = TRUE),
     max_slope = max((slope_calc), na.rm = TRUE),
    min_slope = min((slope_calc),na.rm = TRUE),
    mean_slope = mean((slope_calc), na.rm = TRUE),
     mean_slope100 = mean((height_diff_100 / (n*10))) *100
     # Mean of 'value' column per group
  )


## Descriptive analysis - average and max. slopes over 100m sub segments
results |> 
  st_drop_geometry() |> 
  group_by(type) |> 
  summarise(meanMittelwert = mean(mean_slope100), meanMinimum = min(mean_slope100), meanMaximum = max(mean_slope100),
            meanMax100 = mean(max_slope),
            maxMax100 = max(max_slope), 
            minMin = min(max_slope))



```

## Task 10.1: Segment length

```{r}



## Comparison of length
length_comp <- new |> 
  select(type, messenger, segment_id, rank,  tot_segment_length)


df_spread <- length_comp |> 
  spread(key = type, value = tot_segment_length)


## Compare 'tot_travel' for "original" and "alternative"
df_spread <- df_spread |> 
  mutate(is_original_shorter = original < alternative)

df_spread <- tidyr::spread(length_comp, type, tot_segment_length)

df_spread <- length_comp |> 
  group_by(type, segment_id, rank) |> 
  summarise(
    tot_travel = mean(tot_segment_length)
  )

comparison <- df_spread |> 
  group_by(segment_id) |> 
  summarise(shorter = min(tot_travel[rank == 0]) < min(tot_travel[rank != 0]))

shorter_count <- sum(comparison$shorter)


## Calculate the total number of unique segment_ids
total_segments <- nrow(comparison)


## Calculate the percentage
percentage <- (shorter_count / total_segments) * 100


## alternatives and originals almost 50/50. But original mean tot_segment_length 100m shorter than alternative. Alternative routing based on shortest routes... 




## visualize t.test
ggplot(df_spread, aes(x = rank, y = tot_travel)) +
  geom_boxplot() +
   labs(x = "segment type", y = "total segment length") +
   theme_classic() +
  theme(
    legend.position = "none"
  ) +
  labs(title = "")


# 24 von 57 ids, original was shortest.. im mean 400m kürzer


```

## Task 10.2: width

```{r}


width_preference <- new |> 
  group_by(type, segment_id, rank, width) |> 
  summarise(
    sum = sum(travel_dist, na.rm = T)
  ) 

##Create a complete set of segment IDs and street widths, credits go to: https://stackoverflow.com/questions/73717164/create-all-combinations-of-two-variables-from-two-dataframes-while-keeping-all-o
complete_set <- expand.grid(segment_id = unique(width_preference$segment_id),
                            rank = unique(width_preference$rank),
                           width = as.factor(c(1,2,3,4,6,8)),
                           type = as.factor(c("original", "alternative")))
                           #messenger = as.factor(c("raeubertochter", "donner")))

complete_set <- complete_set |> 
  filter(!(type == "original" & rank != 0)) |> 
  filter(!(type == "alternative" & rank == 0)) 

##Left join this set with your original data frame
complete_df <-  full_join(complete_set, width_preference, by = c("type","segment_id", "rank", "width"))


#Replace NA values with 0
complete_df[is.na(complete_df)] <- 0

    


##
width_preference <- complete_df |> 
   group_by(type, segment_id, rank) |>
  mutate(
    width = width,
    n = n(),
    tot = sum(sum),
    perc = (sum / tot) * 100,
    sum_tot = sum(perc)
  ) 





# Preference$width <- factor(width_preference$width, levels = sort(as.numeric(levels(width_preference$width))))
# 
ggplot(width_preference) +
 geom_boxplot(aes(x = width, y = sum, fill = rank))



ggplot(width_preference) +
  geom_bar(aes(x = width, y = perc, fill = type), stat = "identity", position = "dodge", width = 0.75) +
  scale_fill_manual(values = c("grey10", "grey80")) +
  coord_flip() +
  theme_minimal() +
  labs(x = "street width [m]", y = "percentage [%]", fill = "Type") +
  theme(
    axis.title.y = element_text( margin = margin(t = 0, r = 15, b = 0, l = 0)),
        axis.title.x = element_text( margin = margin(t = 10, r = 0, b = 0, l = 0))
  )





```

## Task 10.3: Slope


# Project work

## Abstract

## Introduction

@laube2011 investigate how temporal scale affects the calculation of movement parameters (speed, sinuosity and turning angle) of animal trajectories.

## Material and Methods

## Results

## Discussion

## Appendix

### Wordcount

<!-- after installing the wordcountadding, remove the line "#| eval: false" -->

```{r}
#| eval: false
wordcountaddin::word_count("index.qmd")
```
